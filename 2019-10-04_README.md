
#김도진

My role for this project was to create an object detection model that can detect drone, car, and person. At the beginning, I wrote a python code that reads a video or webcam and detects objects that are defined in COCO dataset.

--> Did you test with a video? Where did you get the video from? Who recorded video? What objects were there? What do you menas that "objects that are defined in COCO dataset"? The objects in the COCO classese? Then What are in the COCO classes? What objects did you detect from the video exactly?
  
I wrote 3 codes, which are codes that detect objects with pre-trained yolov and yolov3-tiny models using opencv DNN, Pytorch, and Tensorflow.
  
--> What are the combination of the 3 codes? tiny YOLO and tensorflow, tiny YOLO and Pytorch? What are the results of that in terms of training loss, training accuracy, valivation loss, and validation accuracy? What are the accuracy of evaluation result? How deep is the net? What are the parameters of tiny YOLO and YOLOv3? Why do you pick the two ML model in detail (any reference)?
  
Compared performance of each codes and concluded that if not using the GPU while inferring, using opencv DNN is the fastest.

--> Provide graph data of the result. Background must be white, add title in each axis, font size of title be big. Leave the plotting code and input data as a txt file in a folder (make a new folder in this git).

I ended up using yolov3 and yolov3-tiny because it is much faster than faster rcnn or mask rcnn.

--> Why that is faster then rcnns? Explain it in details. And also where the processing is time consuming (inference step or drawing step, etc)

After figuring out what framework to use, I searched for image datasets to train yolov3.

--> Framework here is maybe the ML model and tool configuration?

I found about 2600 drone images from github and about 2600 person & car images from Pascal VOC dataset.

--> Where did you fint the drone images? Please leave the github address here.

I converted Pascal VOC dataset labels into YOLO format.

--> Why and how did you do that? Can't you just use the images rightaway? Where is the code? Explain the process in detail.

While converting to YOLO format, imprecise labels were generated so I looked all images and fixed labels.

--> Why the imprecise labels were generated? Who generated them? How that was happened? How did you fixed the labels?

After dataset was prepared, I used google colab to train custom classes. 
I used google colab because training process required GPU but I did not have one.

--> Why you have to use GPU? Why not only with CPUs? What was the learning rate and how much time required with CPUs for training?

--> Do you think that is reason of CPU and GPU? Not memory? Why did you come up with that you need GPU?
Google colab provided free GPUs for 12hours per session. Training dataset took about 18 hours.<br/>

--> How much of CPUs, GPUs, and memory? How did you come up with trained model when it took 18 hrs and Google provides 12 hrs?

In coming weeks, I will install ros in raspberry pi and upload custom-trained yolo weights, detection codes to the device.

--> Why are you using raspberry pi? What's the reason that you need the device?

I will test how well the model detects objects.

--> How will you test? What is the test procedure for it? 

If the accuracy is low I will figure out other ways to enhance the performance.

--> What accuracy range is "low"? How do you determine that? How would you enhance the performance?

Also, I will integrate camera detetion system with ros nodes. If everything is set I will help radar team for analyzing the SAR image using machine learning. I am not familiar with ros so integrating might be difficult, but I will ask other teammates who are familiar with ros to solve the problem.

--> What is the design of the data flow and data analysis flor of the camera node and camera node related nodes?

--> GREAT!

Below is gif showing how drones are detected using pre-trained yolo drone weight

[Drone-detection](https://i.imgur.com/5UL6AvU.gifv)

--> Rander that as a gif or mp4 or wav or other type and upload the result in this git

Below is repository I am working on for camera detection

[drone_detection Repo](https://github.com/dojinkimm/drone_detection)

--> Sync this repository to a foler (make a folder here for you) with the same name in this repository weekly

#Kyeongnam

My role in this project is to control drones automatically.
I'm going to use DJI Phantom 2(Drone) and UgCS(Ground Station Software).

UgCS: I'm currently researching how to connect laptops-smartphones-DJI Phantom2 while learning how to use UgCS. I'm going to try again because the connection between the laptop and the drone was successful, but the connection between the smartphone and the drone was unsuccessful. Also, I am going to investigate whether I can adjust ground station without my smartphone.

--> What does UgCS provide for the project? Why did you come up with the system? What is the advantages of using the system?
--> Why a smartphone is required? can you also explain how those are connected? Is using a smartphone worth?

DJI Phantom2: I'm learning about drone preparation and drone control with Kar Ee. According to Kar Ee, the drone will have a green light if it's calibration in the area where the GPS is caught. Since I have been banned from flying drones from K-Square's conference room, I plan to fly drone from Professor Tony's farm. I'm going to ask Professor Tony that whether I can fly drones on his farm after learning some basic drone control.

--> I think the best step is 1) practice flying drone with Kal Ee, 2) practice the same in the drone park with Kal Ee, and 3) if needed, ask Tony to use his farm. My suggestion is while you are working on those three steps, try using the simulation to do the same. Make sure the software works fine with the simulation. Then, it should not be a big problem to fly drone using the program.
